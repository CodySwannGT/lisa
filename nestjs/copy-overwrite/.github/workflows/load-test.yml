# This file is managed by Lisa.
# Do not edit directly â€” changes will be overwritten on the next `lisa` run.

name: K6 Load Testing

on:
  workflow_call:
    inputs:
      environment:
        description: 'Target environment to test'
        required: true
        type: string
      test_scenario:
        description: 'Test scenario to run (smoke, load, stress, spike, soak)'
        required: false
        type: string
        default: 'smoke'
      base_url:
        description: 'Base URL of the application to test'
        required: true
        type: string
      k6_version:
        description: 'k6 version to use'
        required: false
        type: string
        default: 'latest'
      test_duration:
        description: 'Override test duration (e.g., 5m, 1h)'
        required: false
        type: string
      virtual_users:
        description: 'Override number of virtual users'
        required: false
        type: number
      thresholds_config:
        description: 'Path to custom thresholds configuration'
        required: false
        type: string
      test_script:
        description: 'Path to custom k6 test script'
        required: false
        type: string
        default: '.github/k6/scripts/default-test.js'
      fail_on_threshold:
        description: 'Fail workflow if thresholds are not met'
        required: false
        type: boolean
        default: true
      upload_results:
        description: 'Upload test results as artifacts'
        required: false
        type: boolean
        default: true
      cloud_run:
        description: 'Run tests on k6 Cloud (requires K6_CLOUD_TOKEN secret)'
        required: false
        type: boolean
        default: false
    outputs:
      test_passed:
        description: 'Whether the test passed all thresholds'
        value: ${{ jobs.k6_test.outputs.passed }}
      results_url:
        description: 'URL to test results (if uploaded)'
        value: ${{ jobs.k6_test.outputs.results_url }}
      summary:
        description: 'Test execution summary'
        value: ${{ jobs.k6_test.outputs.summary }}
    secrets:
      K6_CLOUD_TOKEN:
        required: false
        description: 'k6 Cloud API token for cloud runs'
      CUSTOM_HEADERS:
        required: false
        description: 'Custom headers for authenticated endpoints (JSON format)'

jobs:
  k6_test:
    name: K6 Load Test - ${{ inputs.test_scenario }}
    runs-on: ubuntu-latest
    outputs:
      passed: ${{ steps.run_test.outputs.passed }}
      results_url: ${{ steps.upload_results.outputs.artifact-url }}
      summary: ${{ steps.generate_summary.outputs.summary }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up k6
        run: |
          if [ "${{ inputs.k6_version }}" = "latest" ]; then
            sudo gpg -k
            sudo gpg --no-default-keyring \
              --keyring /usr/share/keyrings/k6-archive-keyring.gpg \
              --keyserver hkp://keyserver.ubuntu.com:80 \
              --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
            echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" \
              | sudo tee /etc/apt/sources.list.d/k6.list
            sudo apt-get update
            sudo apt-get install k6
          else
            K6_VERSION="${{ inputs.k6_version }}"
            wget "https://github.com/grafana/k6/releases/download/v${K6_VERSION}/k6-v${K6_VERSION}-linux-amd64.tar.gz"
            tar -xzf "k6-v${K6_VERSION}-linux-amd64.tar.gz"
            sudo mv "k6-v${K6_VERSION}-linux-amd64/k6" /usr/local/bin/
          fi
          k6 version

      - name: Prepare test environment
        id: prepare_test
        run: |
          # Create results directory
          mkdir -p k6-results

          # Export environment variables for k6
          {
            echo "K6_BASE_URL=${{ inputs.base_url }}"
            echo "K6_SCENARIO=${{ inputs.test_scenario }}"
            echo "K6_ENVIRONMENT=${{ inputs.environment }}"
          } >> "$GITHUB_ENV"

          # Set custom duration if provided
          if [ -n "${{ inputs.test_duration }}" ]; then
            echo "K6_DURATION=${{ inputs.test_duration }}" >> "$GITHUB_ENV"
          fi

          # Set custom VUs if provided
          if [ -n "${{ inputs.virtual_users }}" ]; then
            echo "K6_VUS=${{ inputs.virtual_users }}" >> "$GITHUB_ENV"
          fi

          # Set custom headers if provided
          if [ -n "${{ secrets.CUSTOM_HEADERS }}" ]; then
            echo "K6_CUSTOM_HEADERS=${{ secrets.CUSTOM_HEADERS }}" >> "$GITHUB_ENV"
          fi

          # Set cloud token if cloud run is enabled
          if [ "${{ inputs.cloud_run }}" = "true" ] && [ -n "${{ secrets.K6_CLOUD_TOKEN }}" ]; then
            echo "K6_CLOUD_TOKEN=${{ secrets.K6_CLOUD_TOKEN }}" >> "$GITHUB_ENV"
          fi

      - name: Run k6 test
        id: run_test
        run: |
          set +e  # Don't exit immediately on error

          # Use the shared runner script if a scenario is specified, otherwise use custom script
          if [ "${{ inputs.test_script }}" = ".github/k6/scripts/default-test.js" ]; then
            # Use shared runner for standard scenarios
            RUNNER_ARGS="--scenario ${{ inputs.test_scenario }} --url ${{ inputs.base_url }} --json --csv"

            # Add cloud option if enabled
            if [ "${{ inputs.cloud_run }}" = "true" ]; then
              RUNNER_ARGS="$RUNNER_ARGS --cloud"
            fi

            # Add duration override if provided
            if [ -n "${{ inputs.test_duration }}" ]; then
              RUNNER_ARGS="$RUNNER_ARGS --duration ${{ inputs.test_duration }}"
            fi

            # Add VUs override if provided
            if [ -n "${{ inputs.virtual_users }}" ]; then
              RUNNER_ARGS="$RUNNER_ARGS --vus ${{ inputs.virtual_users }}"
            fi

            # Add no-thresholds flag if needed
            if [ "${{ inputs.fail_on_threshold }}" != "true" ]; then
              RUNNER_ARGS="$RUNNER_ARGS --no-thresholds"
            fi

            echo "Running shared k6 runner: ./scripts/k6-run.sh $RUNNER_ARGS"
            ./scripts/k6-run.sh $RUNNER_ARGS
            TEST_EXIT_CODE=$?
          else
            # Use custom script directly with k6
            TEST_SCRIPT="${{ inputs.test_script }}"

            # Build k6 command
            K6_CMD="k6 run"

            # Add cloud option if enabled
            if [ "${{ inputs.cloud_run }}" = "true" ]; then
              K6_CMD="$K6_CMD --cloud"
            fi

            # Add output options for local runs
            if [ "${{ inputs.cloud_run }}" != "true" ]; then
              K6_CMD="$K6_CMD --out json=k6-results/results.json --out csv=k6-results/results.csv"
            fi

            # Add thresholds config if provided
            if [ -n "${{ inputs.thresholds_config }}" ]; then
              K6_CMD="$K6_CMD --config ${{ inputs.thresholds_config }}"
            fi

            # Add test script
            K6_CMD="$K6_CMD $TEST_SCRIPT"

            echo "Running k6 command: $K6_CMD"
            $K6_CMD
            TEST_EXIT_CODE=$?
          fi

          echo "exit_code=$TEST_EXIT_CODE" >> "$GITHUB_OUTPUT"

          # Determine if test passed
          if [ $TEST_EXIT_CODE -eq 0 ]; then
            echo "passed=true" >> "$GITHUB_OUTPUT"
            echo "âœ… Test passed all thresholds" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "passed=false" >> "$GITHUB_OUTPUT"
            echo "âŒ Test failed thresholds" >> "$GITHUB_STEP_SUMMARY"
          fi

          # Exit with appropriate code based on fail_on_threshold setting
          if [ "${{ inputs.fail_on_threshold }}" = "true" ] && [ $TEST_EXIT_CODE -ne 0 ]; then
            exit $TEST_EXIT_CODE
          fi

      - name: Generate test summary
        id: generate_summary
        if: always()
        run: |
          # Create summary content
          SUMMARY="# K6 Load Test Results\n\n"
          SUMMARY+="**Environment:** ${{ inputs.environment }}\n"
          SUMMARY+="**Test Scenario:** ${{ inputs.test_scenario }}\n"
          SUMMARY+="**Base URL:** ${{ inputs.base_url }}\n"
          SUMMARY+="**Test Status:** "

          if [ "${{ steps.run_test.outputs.passed }}" = "true" ]; then
            SUMMARY+="âœ… PASSED\n"
          else
            SUMMARY+="âŒ FAILED\n"
          fi

          # Add summary to GitHub Step Summary
          echo -e "$SUMMARY" >> "$GITHUB_STEP_SUMMARY"

          # Extract key metrics from results if available
          if [ -f "k6-results/results.json" ]; then
            echo -e "\n## Key Metrics\n" >> "$GITHUB_STEP_SUMMARY"

            # Parse JSON results for key metrics (simplified example)
            # In a real implementation, you'd use jq or similar to extract detailed metrics
            echo "ðŸ“Š Detailed metrics available in artifacts" >> "$GITHUB_STEP_SUMMARY"
          fi

          # Output summary for workflow output
          {
            echo "summary<<EOF"
            echo -e "$SUMMARY"
            echo "EOF"
          } >> "$GITHUB_OUTPUT"

      - name: Upload test results
        id: upload_results
        if: always() && inputs.upload_results
        uses: actions/upload-artifact@v4
        with:
          name: k6-results-${{ inputs.environment }}-${{ inputs.test_scenario }}-${{ github.run_id }}
          path: k6-results/
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const summary = `${{ steps.generate_summary.outputs.summary }}`;
            const resultsUrl = '${{ steps.upload_results.outputs.artifact-url }}';

            let comment = summary;
            if (resultsUrl) {
              comment += `\n\n[ðŸ“Š View detailed results](${resultsUrl})`;
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
